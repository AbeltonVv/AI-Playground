llamaCppConfig = {
    "ggufLLM": "../service/models/llm/ggufLLM",
    "llamaCPPEmbedding": "../service/models/embedding/llamaCPP",
}

device = "xpu"
